# -*- coding: utf-8 -*-
"""Thin_blood_smear_slide_images_for_diagnosis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n2A6q3rAQ0xYoFgqOEQkTOGqlrUVpq6Y
"""

import tensorflow as tf
 import numpy as np
 import matplotlib.pyplot as plt
 import tensorflow_datasets as tfds
 from tensorflow.keras.layers import InputLayer, Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization
 from tensorflow.keras.losses import BinaryCrossentropy
 from tensorflow.keras.optimizers import Adam

dataset, dataset_info = tfds.load('malaria', with_info = True, as_supervised = True, shuffle_files = True, split = ['train'])

dataset

dataset_info

for data in dataset[0].take(5):
  print(data)

"""**Splitting the dataset into train , validation and test datasets\**"""

# split_dataset function split the dataset into train, validation and test sets
def split_dataset(dataset, train_ratio, val_ratio, test_ratio):
  dataset_size = len(dataset)

  train_dataset = dataset.take(int(train_ratio * dataset_size))

  val_test_dataset = dataset.skip(int(train_ratio * dataset_size))
  val_dataset = val_test_dataset.take(int(val_ratio * dataset_size))

  test_dataset = val_test_dataset.skip(int(test_ratio * dataset_size))
  return train_dataset, val_dataset, test_dataset

train_ratio = 0.8
val_ratio = 0.1
test_ratio = 0.1

train_dataset, val_dataset, test_dataset = split_dataset(dataset[0], train_ratio, val_ratio, test_ratio )
print(list(train_dataset.take(1).as_numpy_iterator()), list(val_dataset.take(1).as_numpy_iterator()), list(test_dataset.take(1).as_numpy_iterator()) )

"""**Data Visualization :**
Visualizing some images along with their labels

"""

# Take first 16 images from the train dataset
for i, (image, label) in enumerate(train_dataset.take(16)):
  ax = plt.subplot(4,4, i+1)
  plt.imshow(image)
  plt.title(dataset_info.features['label'].int2str(label))
  plt.axis('off')

"""**Data Preprocessing**


1. The images need to be resized to a single size.

2. The pixel data also needs to be normalized.


    Normalization = (X - Xmin)/ (Xmax - Xmin)

    In image Xmin = 0, Xmax= 255
"""

# Function to resize and rescale the images
im_size = 224
def resizing_rescale_images(image, label):
  return tf.image.resize(image, (im_size, im_size))/255.0, label

train_dataset = train_dataset.map(resizing_rescale_images)
val_dataset = val_dataset.map(resizing_rescale_images)
test_dataset = test_dataset.map(resizing_rescale_images)
train_dataset

for image, label in train_dataset.take(1):
  print(image,label)

#using tf.data API to prefetch data for faster loading of data
batch_size = 32
train_dataset = train_dataset.shuffle(buffer_size=8, reshuffle_each_iteration= True).batch(batch_size).prefetch(tf.data.AUTOTUNE)
train_dataset

val_dataset = val_dataset.shuffle(buffer_size=8, reshuffle_each_iteration= True).batch(batch_size).prefetch(tf.data.AUTOTUNE)
val_dataset

"""**CREATING THE MODEL USING KERAS SEQUENTIAL API**

  As we are dealing with images as input, convolution neural network will be used for the classification task.The first few layers would be the convolution layers which would have certain filters that extracts feature information from the input images and sends it through an activation function to create the feature maps.These are then sent through batch normalization layer that promises faster training and easier learning alowing higher learning rates. These feature maps are then passed through a pooling layer. After certain convolution and pooling layers, their output is flattened and passed through a deep neural network to classify the images into their corresponding category. The output of the overall model would be a probability distribution providing the probability of each category and the class with highest probability is selected as the result of the model.
"""

model= tf.keras.Sequential([
    InputLayer(input_shape = (im_size,im_size,3)),

    Conv2D(filters=6, kernel_size = 3, strides=1, padding='valid', activation='relu'),
    BatchNormalization(),
    MaxPool2D(pool_size = 2, strides=2),

    Conv2D(filters=16, kernel_size = 3, strides=1, padding='valid', activation='relu'),
    BatchNormalization(),
    MaxPool2D(pool_size = 2, strides=2),



    Flatten(),

    Dense(100,activation='relu'),
    BatchNormalization(),

    Dense(10, activation='relu'),
    BatchNormalization(),

    Dense(1, activation='sigmoid'),


])

model.summary()

tf.keras.utils.plot_model(model, to_file = "model.png", show_shapes = True)

"""**Compiling the model**

    Optimizer: Adam
    learning rate:0.1
    Loss: BinaryCrossentropy
    Evaluation metric: Accuracy
"""

model.compile(
    optimizer= Adam(learning_rate=0.01),
    loss= BinaryCrossentropy(),
    metrics = 'accuracy'
)

"""**Training the model**"""

trained_model = model.fit(train_dataset, validation_data=val_dataset, epochs=50, verbose=1)

"""**Visualizing the model loss and performance**"""

plt.plot(trained_model.history['loss'])
plt.plot(trained_model.history['val_loss'])
plt.title('Model loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train_loss','validation_loss'])
plt.show()

plt.plot(trained_model.history['accuracy'])
plt.plot(trained_model.history['val_accuracy'])
plt.title('Model Performance')
plt.xlabel('epoch')
plt.ylabel('Accuracy')
plt.legend(['Train_accuracy','Val_accuracy'])
plt.show()

"""**Evaluating and testing the model**"""

test_dataset = test_dataset.batch(1)
test_dataset

model.evaluate(test_dataset)

model.predict(test_dataset.take(1))[0][0]

# Function that sets a condition for whether the predicted probability leads to an infected(I) or not infected(NI)
def infected_or_not(x):
  if(x<0.5):
    return str('I')
  else:
    return str('NI')

# Visualizing the actual vs predicted class
for i, (image,label) in enumerate(test_dataset.take(12)):
  ax= plt.subplot(3,4, i+1)
  plt.imshow(image[0])
  plt.title(str(infected_or_not(label.numpy()[0])) + ":" + str(infected_or_not(model.predict(image)[0][0])))
  plt.axis('off')

